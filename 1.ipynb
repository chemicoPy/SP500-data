{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chemicoPy/SP500-data/blob/main/1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8pJG_QHK_aC"
      },
      "source": [
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xQLiGhILMwL",
        "outputId": "faa210e5-51d9-41e2-e276-1336082492b4"
      },
      "source": [
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao8Idl-yLdmb"
      },
      "source": [
        "\n",
        "!pip install redis\n",
        "!pip install yfinance\n",
        "!pip install yahoo_fin\n",
        "!pip install alpha_vantage\n",
        "!pip install requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKcVkgL5MjGF"
      },
      "source": [
        "\n",
        "import redis\n",
        "import yfinance as yf\n",
        "import yahoo_fin.stock_info as si\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import date, timedelta\n",
        "import csv\n",
        "import alpha_vantage\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "import warnings\n",
        "import json\n",
        "import plotly.graph_objs as go\n",
        "import requests\n",
        "from alpha_vantage.techindicators import TechIndicators\n",
        "import os\n",
        "import io\n",
        "import glob\n",
        "import yahoo_fin.stock_info as si\n",
        "from yahoo_fin.stock_info import get_data, tickers_sp500, tickers_nasdaq, tickers_other, get_quote_table\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozzl8vCn57ej"
      },
      "source": [
        "sp_500 = si.tickers_sp500()\n",
        "\n",
        "datasp_500 = sp_500\n",
        "\n",
        "data1 = list(datasp_500[:50])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9Uum-yI7C3F"
      },
      "source": [
        "data1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLL2ikFj7Bke"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIs6zwuKONZD"
      },
      "source": [
        "API_key = 'SPL5TR7O541X3K5B'\n",
        "\n",
        "# Extra API_key = T0S6DRN24YLDCJFK\n",
        "\n",
        "ts = TimeSeries(key=API_key, output_format='csv')\n",
        "\n",
        "tickers_list = data1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKdh65CerbVc"
      },
      "source": [
        "\"\"\" For 1min \"\"\"\n",
        "\n",
        "allUrlData_1min = {}\n",
        "for ticker in tickers_list:\n",
        "\n",
        "    for yr in range(2):\n",
        "        for mo in range(12):\n",
        "            allUrlData_1min[ticker] = ts.get_intraday_extended(ticker, interval='1min', slice='year'+str(yr+1)+'month'+str(mo+1))\n",
        "    \n",
        "            with open(ticker+' (1min).csv', 'a', newline='') as write_csvfile:\n",
        "                writer = csv.writer(write_csvfile, dialect='excel')\n",
        "                for row in allUrlData_1min[ticker][0]:\n",
        "                    writer.writerow(row)\n",
        "    \n",
        "            time.sleep(15)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfq0Tdl9XrfS"
      },
      "source": [
        "\"\"\" For 5min \"\"\"\n",
        "\n",
        "allUrlData_5min = {}\n",
        "for ticker in tickers_list:\n",
        "\n",
        "    for yr in range(2):\n",
        "        for mo in range(12):\n",
        "            allUrlData_5min[ticker] = ts.get_intraday_extended(ticker, interval='5min', slice='year'+str(yr+1)+'month'+str(mo+1))\n",
        "    \n",
        "            with open(ticker+' (5min).csv', 'a', newline='') as write_csvfile:\n",
        "                writer = csv.writer(write_csvfile, dialect='excel')\n",
        "                for row in allUrlData_5min[ticker][0]:\n",
        "                    writer.writerow(row)\n",
        "    \n",
        "            time.sleep(15)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFnCeAHOXvFv"
      },
      "source": [
        "\"\"\" For 30min \"\"\"\n",
        "\n",
        "allUrlData_30min = {}\n",
        "for ticker in tickers_list:\n",
        "\n",
        "    for yr in range(2):\n",
        "        for mo in range(12):\n",
        "            allUrlData_30min[ticker] = ts.get_intraday_extended(ticker, interval='30min', slice='year'+str(yr+1)+'month'+str(mo+1))\n",
        "            \n",
        "            with open(ticker+' (30min).csv', 'a', newline='') as write_csvfile:\n",
        "                writer = csv.writer(write_csvfile, dialect='excel')\n",
        "                for row in allUrlData_30min[ticker][0]:\n",
        "                    writer.writerow(row)\n",
        "    \n",
        "            time.sleep(15)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh5dVL_KftJ5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSBnsMeghaEc"
      },
      "source": [
        "\"\"\" For Daily data \"\"\"\n",
        "\n",
        "allUrlData_daily = {}\n",
        "for ticker in tickers_list:\n",
        "\n",
        "    for yr in range(2):\n",
        "        for mo in range(12):\n",
        "            allUrlData_daily[ticker] = ts.get_daily(ticker, outputsize=\"full\")\n",
        "            \n",
        "            with open(ticker+\" (Daily data).csv\", 'a', newline='') as write_csvfile:\n",
        "                writer = csv.writer(write_csvfile, dialect='excel')\n",
        "                for row in allUrlData_daily[ticker][0]:\n",
        "                    writer.writerow(row)\n",
        "    \n",
        "            time.sleep(15)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fuM2p9Kzfe3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}