{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chemicoPy/SP500-data/blob/main/2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyJ0hGyY7LSw"
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W2dCtoO-AO4",
        "outputId": "95291917-0697-42a9-db5b-264c5def584c"
      },
      "source": [
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q16Y2ZE2-Amr"
      },
      "source": [
        "\n",
        "!pip install redis\n",
        "!pip install yfinance\n",
        "!pip install yahoo_fin\n",
        "!pip install alpha_vantage\n",
        "!pip install requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79P1uEtT-Ato"
      },
      "source": [
        "\n",
        "import redis\n",
        "import yfinance as yf\n",
        "import yahoo_fin.stock_info as si\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import date, timedelta\n",
        "import csv\n",
        "import alpha_vantage\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "import warnings\n",
        "import json\n",
        "import plotly.graph_objs as go\n",
        "import requests\n",
        "from alpha_vantage.techindicators import TechIndicators\n",
        "import os\n",
        "import io\n",
        "import glob\n",
        "import yahoo_fin.stock_info as si\n",
        "from yahoo_fin.stock_info import get_data, tickers_sp500, tickers_nasdaq, tickers_other, get_quote_table\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bFt_Aw1-YmP"
      },
      "source": [
        "sp_500 = si.tickers_sp500()\n",
        "\n",
        "datasp_500 = sp_500\n",
        "\n",
        "\n",
        "data2 = list(datasp_500[50:100])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7deVGlWbKObI"
      },
      "source": [
        "data2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTrReOFC-dPu"
      },
      "source": [
        "API_key = 'ZW4QWY71MGYRQFZS'\n",
        "\n",
        "#Extra API_key = WBCYZBGUBIFA5Y46\n",
        "\n",
        "ts = TimeSeries(key=API_key, output_format='csv')\n",
        "\n",
        "tickers_list = data2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnSdHKkN-vO_"
      },
      "source": [
        "\"\"\" For 1min \"\"\"\n",
        "\n",
        "allUrlData_1min = {}\n",
        "for ticker in tickers_list:\n",
        "\n",
        "    for yr in range(2):\n",
        "        for mo in range(12):\n",
        "            allUrlData_1min[ticker] = ts.get_intraday_extended(ticker, interval='1min', slice='year'+str(yr+1)+'month'+str(mo+1))\n",
        "    \n",
        "            with open(ticker+' (1min).csv', 'a', newline='') as write_csvfile:\n",
        "                writer = csv.writer(write_csvfile, dialect='excel')\n",
        "                for row in allUrlData_1min[ticker][0]:\n",
        "                    writer.writerow(row)\n",
        "    \n",
        "            time.sleep(15)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVeraHIn-wMu"
      },
      "source": [
        "\"\"\" For 5min \"\"\"\n",
        "\n",
        "allUrlData_5min = {}\n",
        "for ticker in tickers_list:\n",
        "\n",
        "    for yr in range(2):\n",
        "        for mo in range(12):\n",
        "            allUrlData_5min[ticker] = ts.get_intraday_extended(ticker, interval='5min', slice='year'+str(yr+1)+'month'+str(mo+1))\n",
        "    \n",
        "            with open(ticker+' (5min).csv', 'a', newline='') as write_csvfile:\n",
        "                writer = csv.writer(write_csvfile, dialect='excel')\n",
        "                for row in allUrlData_5min[ticker][0]:\n",
        "                    writer.writerow(row)\n",
        "    \n",
        "            time.sleep(15)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo5xZyU6-yWq"
      },
      "source": [
        "\"\"\" For 30min \"\"\"\n",
        "\n",
        "allUrlData_30min = {}\n",
        "for ticker in tickers_list:\n",
        "\n",
        "    for yr in range(2):\n",
        "        for mo in range(12):\n",
        "            allUrlData_30min[ticker] = ts.get_intraday_extended(ticker, interval='30min', slice='year'+str(yr+1)+'month'+str(mo+1))\n",
        "            \n",
        "            with open(ticker+' (30min).csv', 'a', newline='') as write_csvfile:\n",
        "                writer = csv.writer(write_csvfile, dialect='excel')\n",
        "                for row in allUrlData_30min[ticker][0]:\n",
        "                    writer.writerow(row)\n",
        "    \n",
        "            time.sleep(15)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k616jg1m-1KE"
      },
      "source": [
        "\"\"\" For Daily data \"\"\"\n",
        "\n",
        "allUrlData_daily = {}\n",
        "for ticker in tickers_list:\n",
        "\n",
        "    for yr in range(2):\n",
        "        for mo in range(12):\n",
        "            allUrlData_daily[ticker] = ts.get_daily(ticker, outputsize=\"full\")\n",
        "            \n",
        "            with open(ticker+\" (Daily data).csv\", 'a', newline='') as write_csvfile:\n",
        "                writer = csv.writer(write_csvfile, dialect='excel')\n",
        "                for row in allUrlData_daily[ticker][0]:\n",
        "                    writer.writerow(row)\n",
        "    \n",
        "            time.sleep(15)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fydaZeF-7pY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}