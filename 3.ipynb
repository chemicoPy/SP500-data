{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chemicoPy/SP500-data/blob/main/3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i4BhVWj82OI"
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxbvCxe8_L3F",
        "outputId": "8d0f68d3-903c-4845-b310-7073b0e5f25b"
      },
      "source": [
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBqVLOba_Oa9"
      },
      "source": [
        "\n",
        "!pip install redis\n",
        "!pip install yfinance\n",
        "!pip install yahoo_fin\n",
        "!pip install alpha_vantage\n",
        "!pip install requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aFIII76_ZcU"
      },
      "source": [
        "\n",
        "import redis\n",
        "import yfinance as yf\n",
        "import yahoo_fin.stock_info as si\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import date, timedelta\n",
        "import csv\n",
        "import alpha_vantage\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "import warnings\n",
        "import json\n",
        "import plotly.graph_objs as go\n",
        "import requests\n",
        "from alpha_vantage.techindicators import TechIndicators\n",
        "import os\n",
        "import io\n",
        "import glob\n",
        "import yahoo_fin.stock_info as si\n",
        "from yahoo_fin.stock_info import get_data, tickers_sp500, tickers_nasdaq, tickers_other, get_quote_table\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzni9m0D_alR"
      },
      "source": [
        "sp_500 = si.tickers_sp500()\n",
        "\n",
        "datasp_500 = sp_500\n",
        "\n",
        "\n",
        "data3 = list(datasp_500[100:150])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVsrGvo7LlwP"
      },
      "source": [
        "data3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHtqy1my_fH7"
      },
      "source": [
        "API_key = 'T0S6DRN24YLDCJFK'\n",
        "\n",
        "#Extra API_key = SPL5TR7O541X3K5B\n",
        "\n",
        "ts = TimeSeries(key=API_key, output_format='csv')\n",
        "\n",
        "tickers_list = data3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELv65V4S_rf_"
      },
      "source": [
        "\"\"\" For 1min \"\"\"\n",
        "\n",
        "allUrlData_1min = {}\n",
        "for ticker in tickers_list:\n",
        "\n",
        "    for yr in range(2):\n",
        "        for mo in range(12):\n",
        "            allUrlData_1min[ticker] = ts.get_intraday_extended(ticker, interval='1min', slice='year'+str(yr+1)+'month'+str(mo+1))\n",
        "    \n",
        "            with open(ticker+' (1min).csv', 'a', newline='') as write_csvfile:\n",
        "                writer = csv.writer(write_csvfile, dialect='excel')\n",
        "                for row in allUrlData_1min[ticker][0]:\n",
        "                    writer.writerow(row)\n",
        "    \n",
        "            time.sleep(15)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4B8TM3p_uAz"
      },
      "source": [
        "\"\"\" For 5min \"\"\"\n",
        "\n",
        "allUrlData_5min = {}\n",
        "for ticker in tickers_list:\n",
        "\n",
        "    for yr in range(2):\n",
        "        for mo in range(12):\n",
        "            allUrlData_5min[ticker] = ts.get_intraday_extended(ticker, interval='5min', slice='year'+str(yr+1)+'month'+str(mo+1))\n",
        "    \n",
        "            with open(ticker+' (5min).csv', 'a', newline='') as write_csvfile:\n",
        "                writer = csv.writer(write_csvfile, dialect='excel')\n",
        "                for row in allUrlData_5min[ticker][0]:\n",
        "                    writer.writerow(row)\n",
        "    \n",
        "            time.sleep(15)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkB_nEqn_wTY"
      },
      "source": [
        "\"\"\" For 30min \"\"\"\n",
        "\n",
        "allUrlData_30min = {}\n",
        "for ticker in tickers_list:\n",
        "\n",
        "    for yr in range(2):\n",
        "        for mo in range(12):\n",
        "            allUrlData_30min[ticker] = ts.get_intraday_extended(ticker, interval='30min', slice='year'+str(yr+1)+'month'+str(mo+1))\n",
        "            \n",
        "            with open(ticker+' (30min).csv', 'a', newline='') as write_csvfile:\n",
        "                writer = csv.writer(write_csvfile, dialect='excel')\n",
        "                for row in allUrlData_30min[ticker][0]:\n",
        "                    writer.writerow(row)\n",
        "    \n",
        "            time.sleep(15)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW3Dacqc_ybH"
      },
      "source": [
        "\"\"\" For Daily data \"\"\"\n",
        "\n",
        "allUrlData_daily = {}\n",
        "for ticker in tickers_list:\n",
        "\n",
        "    for yr in range(2):\n",
        "        for mo in range(12):\n",
        "            allUrlData_daily[ticker] = ts.get_daily(ticker, outputsize=\"full\")\n",
        "            \n",
        "            with open(ticker+\" (Daily data).csv\", 'a', newline='') as write_csvfile:\n",
        "                writer = csv.writer(write_csvfile, dialect='excel')\n",
        "                for row in allUrlData_daily[ticker][0]:\n",
        "                    writer.writerow(row)\n",
        "    \n",
        "            time.sleep(15)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaS4yGcV_0kO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}